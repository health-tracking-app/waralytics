import ssl
import requests
from requests.adapters import HTTPAdapter, Retry
import requests_html
from sqlalchemy import create_engine
import imutils
import re
from datetime import datetime
from dateutil import parser
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import cv2
from pytesseract import pytesseract


class WebParser:
    """
    Class to take a web page from Oryx blog and parse from it details about the arms losses.
    """

    def __init__(self, web_url: str, js_content: bool = False, sleep=5):
        """
        :param web_url: URL from the Oryx website to parse
        :param js_content: Does webpage contain JS dynamically generated content?
        :param: sleep: How much time it takes to render JS dynamically generated content?
        """

        self.web_url = web_url
        self.js_content = js_content
        self.sleep = sleep

        # Web page's source code (unparsed)
        self.html_page = None
        # Web page's source code (parsed)
        self.html_soup = None
        # Dictionary with losses summary (from headings summaries)
        self.loss_sum_dict = None
        # List of all <ul> on the web page
        self.loss_det_raw = None
        # List of <ul> with information about the losses
        self.loss_det_filtered = None
        # Data frame with losses details (before expanding multiple losses rows into several lines)
        self.df_loss_raw = None

        # On class initiation parse a web page
        if self.js_content:
            self.parse_webpage_with_js()
        else:
            self.parse_webpage()

    def get_html_page(self, web_url, js_content=False, server_response=False, num_retries=5, backoff_factor=0.1,
                      timeout=10):

        # Create a session (for pages with dynamically generated
        # content we need a special type of session)
        if js_content:
            session = requests_html.HTMLSession()
        else:
            session = requests.Session()

        # Define connection retries configuration
        retries = Retry(total=num_retries, backoff_factor=backoff_factor)

        # Mount config to our session
        session.mount('http://', HTTPAdapter(max_retries=retries))

        if server_response:
            # Get only server response for a web page
            response = session.head(web_url, timeout=timeout)
        else:
            # Get full html content of a web page
            response = session.get(web_url, timeout=timeout)

        return response

    def parse_webpage(self):

        self.html_page = self.get_html_page(self.web_url).content
        self.html_soup = BeautifulSoup(self.html_page, "html.parser")

    def parse_webpage_with_js(self):

        # Twitter contains dynamically generated by JS content, which cannot be extracted by usual
        # requests.get() call. We can either use selenium to get that info (but it's heavy) or
        # intercept additional XHR call to twitter API that client makes (Google Chrome => Dev Tools =>
        # Network => Fetch/XHR => "TweetDetail?variables[...]" => Response) (but we need to know
        # an exact url for that) or use request-html (which uses chronium under the hood but IMHO
        # is easier to deal with than selenium)

        self.html_page = self.get_html_page(self.web_url, js_content=True)

        # We need to wait a bit before page fully renders
        self.html_page.html.render(sleep=self.sleep)
        self.html_soup = BeautifulSoup(self.html_page.html.html, "html.parser")

    def extract_summary(self):

        # Losses summary are placed inside <h3>
        loss_sum_raw = self.html_soup.find_all('h3')

        # Data cleansing

        # Get rid of empty strings
        loss_sum = [i.text.strip() for i in loss_sum_raw if i.text.strip() != ""]

        # Get rid of unnecessary words/punctuation and standardize format
        # TODO: One-time exceptions - standardizing is likely an overkill
        loss_sum = [i.replace("of which:", "").replace("of which", "").replace(" And ", " and ") for i in loss_sum]
        loss_sum = [i.replace("Trucks, Vehicles and Jeeps", "Trucks and Vehicles and Jeeps") for i in loss_sum]
        loss_sum = [i.replace("(MRAP) ", "") for i in loss_sum]
        loss_sum = [i.replace(" - ", ":").replace("(", ":").replace(")", "") for i in loss_sum]

        # Convert list of strings into a list of dictionaries
        loss_sum_dict = [dict(i.split(":") for i in j.split(",")) for j in loss_sum]
        # Get rid of leading/trailing spaces in keys and values
        self.loss_sum_dict = [{a.strip(): b.strip() for a, b in i.items()} for i in loss_sum_dict]

        # TODO: Finish task - convert dict into a data frame

    def extract_details(self):

        # Losses details are stored inside <ul>
        self.loss_det_raw = self.html_soup.find_all("ul")

        # Get rid of non-war losses details

        def det_filter(elm):
            pattern_1 = elm.next_element.name == "li" and elm.next_element.next_element.name == "img"
            pattern_2 = elm.next_element.name == "li" and elm.next_element.next_element.name == "span"
            result = pattern_1 or pattern_2
            return result

        self.loss_det_filtered = [i for i in self.loss_det_raw if det_filter(i)]

        # Construct a list with losses details

        # Supplementary functions

        def find_arm_producer(txt):
            txt = txt.lower()
            arm_prd = ""
            if "soviet" in txt and "union" in txt:
                arm_prd = "USSR"
            elif "russia" in txt:
                arm_prd = "Russian Federation"
            elif "belarus" in txt:
                arm_prd = "Belarus"
            elif "italy" in txt:
                arm_prd = "Italy"
            elif "czech" in txt and "republic" in txt:
                arm_prd = "Czechia"
            elif "israel" in txt:
                arm_prd = "Israel"
            elif "poland" in txt:
                arm_prd = "Poland"
            elif "ukraine" in txt:
                arm_prd = "Ukraine"
            elif ("united" in txt and "kingdom" in txt) or "britain" in txt:
                arm_prd = "United Kingdom"
            elif "united" in txt and "states" in txt:
                arm_prd = "United States"
            elif "turkey" in txt:
                arm_prd = "Turkey"
            elif "germany" in txt:
                arm_prd = "Germany"
            return arm_prd

        def find_arm_owner(html_ul):

            # Default arm_category
            arm_own = ""

            # Generator to get all the previous siblings - more effective search
            gen_sib = html_ul.previous_siblings

            # Pattern for arm_owner
            # 'Russia - 2899, of which: destroyed: 1548, damaged: 44, abandoned: 237, captured: 1070'

            for sib in gen_sib:
                if sib.name == "h3" and "russia" in sib.text.lower() and "of which" in sib.text.lower():
                    arm_own = "Russian Federation"
                    break
                elif sib.name == "h3" and "ukraine" in sib.text.lower() and "of which" in sib.text.lower():
                    arm_own = "Ukraine"
                    break

            if arm_own == "":

                # In some cases <h3> and <ul> are encircled in <div>
                # In that case <ul> has only one previous sibling - <h3> with arm_category
                # Therefore, to find arm_own we need to loop through all the previous elements
                # and not just siblings

                # Generator to get all the previous elements - less effective search
                gen_elm = html_ul.previous_elements

                for sib in gen_elm:
                    if sib.name == "h3" and "russia" in sib.text.lower() and "of which" in sib.text.lower():
                        arm_own = "Russian Federation"
                        break
                    elif sib.name == "h3" and "ukraine" in sib.text.lower() and "of which" in sib.text.lower():
                        arm_own = "Ukraine"
                        break

            return arm_own

        def find_arm_category(html_li):

            # Default arm_category
            arm_ctg = ""

            # Generator to get all the previous siblings
            gen_sib = html_li.previous_siblings

            # Pattern for arm_category
            # 'Tanks (507, of which destroyed: 257, damaged: 9, abandoned: 40, captured: 201)'
            arm_ctg_pattern_1 = re.compile(r"^\w*.*\(\d*.*of which")
            arm_ctg_pattern_2 = re.compile(r"\(\d+.*of which")

            # We are doing search up to 4 levels back, otherwise something is wrong
            i = 4

            try:
                while i > 0:
                    sib = next(gen_sib)
                    if sib.name == "h3" and sib.text.strip() != "":
                        arm_ctg_match_1 = arm_ctg_pattern_1.search(sib.text.strip())
                        if arm_ctg_match_1 is None:
                            pass
                        else:
                            # 'Tanks (507, of which'
                            arm_ctg_txt = arm_ctg_match_1.group()
                            # '(507, of which'
                            arm_ctg_pos = arm_ctg_pattern_2.search(arm_ctg_txt).start()
                            # "Tanks"
                            arm_ctg = arm_ctg_txt[:arm_ctg_pos].strip()
                            break
                    i -= 1
            except StopIteration:
                pass

            arm_ctg = arm_ctg.title().replace("'", "")

            return arm_ctg

        def find_action_type(txt):
            action_type = txt
            txt = txt.lower()
            if "destroyed" in txt:
                action_type = "Destroyed"
            elif "burned" in txt:
                action_type = "Destroyed"
            elif "sunk" in txt:
                action_type = "Sunk"
            elif "scuttled" in txt:
                action_type = "Sunk"
            # What about situation when firstly captured and then abandoned?
            elif "captured" in txt and "abandoned" in txt and txt.find("captured") < txt.find("abandoned"):
                action_type = "Abandoned"
            elif "captured" in txt:
                action_type = "Captured"
            elif "abandoned" in txt:
                action_type = "Abandoned"
            elif "damaged" in txt:
                action_type = "Damaged"
            return action_type

        def find_arm_type(txt):
            txt = txt.lower()
            if "engineer" in txt:
                arm_tp = "Engineering"
            elif 'communication' in txt or 'radar' in txt or 'jammer' in txt:
                arm_tp = 'Communication'
            elif 'logistic' in txt or 'truck' in txt:
                arm_tp = 'Logistics'
            elif 'medical' in txt:
                arm_tp = 'Medical'
            else:
                arm_tp = 'Battle'
            return arm_tp

        def find_number_ids(txt):
            txt = txt.lower()
            # Count a number of numbers in an input string (i.e. number of losses mentioned in one row)
            regex_pattern = re.compile(r"(^\d*[ ,])|([, ]\d*[ ,])")
            regex_match = re.findall(regex_pattern, txt)
            if len(regex_match) == 0:
                number_ids = 1
            else:
                number_ids = len(regex_match)
            return number_ids

        # Loop through the list of losses and construct a list with irs details

        list_loss = []

        for elm_ul in self.loss_det_filtered:
            arm_owner = find_arm_owner(elm_ul)
            arm_category = find_arm_category(elm_ul)
            arm_type = find_arm_type(arm_category)
            for elm_li in elm_ul:
                arm_producer = find_arm_producer(elm_li.img["src"])
                arm_model = elm_li.text.strip()[elm_li.text.strip().find(" ") + 1:
                                                elm_li.text.strip().find(":")].title().replace("'", "")
                for elm_a in elm_li.find_all("a"):
                    arm_id_action = elm_a.text.strip(" ()").replace("'", "")   # elm_a.text[elm_a.text.find(",") + 1:].strip(" )")
                    arm_action = find_action_type(arm_id_action)
                    arm_num_id = find_number_ids(arm_id_action)
                    arm_photo_url_original = elm_a["href"].strip()
                    if "twitter" in arm_photo_url_original:
                        # Link leads to twitter - we extract date from web page's metadata
                        arm_photo_url_final = arm_photo_url_original
                    elif "." in arm_photo_url_original[-6:]:
                        # Link leads to a picture (i.e. .jpg) - we extract date from an image
                        arm_photo_url_final = arm_photo_url_original
                    else:
                        # Link leads to a page that contains an image - we need to get picture's url itself first
                        # and later recognize date from it
                        try:
                            redirect_web_page = self.get_html_page(arm_photo_url_original).content
                            redirect_bs = BeautifulSoup(redirect_web_page, "html.parser")
                            # print(arm_photo_url_original)
                            arm_photo_url_final = redirect_bs.find(id="main-image")['src'].strip()
                        except Exception:
                            arm_photo_url_final = "Error on connecting to original url or parsing webpage"
                    elm_list_loss = [arm_type, arm_category, arm_model, arm_id_action, arm_action, arm_num_id,
                                     arm_photo_url_original, arm_photo_url_final, arm_producer, arm_owner]
                    list_loss.append(elm_list_loss)

        df_columns = ["Equipment Type", "Equipment Category", "Equipment Model", "Action ID & Types", "Action Type",
                      "Number of IDs", "Source Link Original", "Source Link Final", "Equipment Producer",
                      "Impacted Country"]

        self.df_loss_raw = pd.DataFrame(data=list_loss, columns=df_columns)

        # Get rid of duplicate lines caused by error in the html structure of source web page
        # TODO: One-time exception - see no way to standardize
        self.df_loss_raw.drop(self.df_loss_raw[(self.df_loss_raw["Equipment Category"] == "Towed Artillery") &
                                               (self.df_loss_raw["Equipment Model"].str.lower() == "152mm 2a65 msta-b howitzer") &
                                               # (self.df_loss_raw["Action ID & Types"] == "13,") &
                                               (self.df_loss_raw["Source Link Original"] == "https://postimg.cc/q6CYJkkd") &
                                               (self.df_loss_raw["Impacted Country"] == "Russian Federation")]
                              .index, inplace=True)
        # self.df_loss_raw.drop(self.df_loss_raw[(self.df_loss_raw["Equipment Category"] == "Towed Artillery") &
        #                                        (self.df_loss_raw["Equipment Model"].str.lower() == "152mm 2a65 msta-b howitzer") &
        #                                        (self.df_loss_raw["Action ID & Types"] == "") &
        #                                        (self.df_loss_raw["Source Link Original"] == "https://postimg.cc/q6CYJkkd") &
        #                                        (self.df_loss_raw["Impacted Country"] == "Russian Federation")]
        #                       .index, inplace=True)
        row_to_change = self.df_loss_raw[(self.df_loss_raw["Equipment Category"] == "Towed Artillery") &
                                         (self.df_loss_raw["Equipment Model"].str.lower() == "152mm 2a65 msta-b howitzer") &
                                         (self.df_loss_raw["Action ID & Types"].str.lower() == "damaged by bayraktar tb2") &
                                         (self.df_loss_raw["Source Link Original"] == "https://i.postimg.cc/yYx8J43v/Screenshot-8073.png") &
                                         (self.df_loss_raw["Impacted Country"] == "Russian Federation")].index
        if not row_to_change.empty:
            # Attempt to change only for Russia to avoid index out of range
            self.df_loss_raw.at[row_to_change[0], "Action ID & Types"] = "13, damaged by Bayraktar TB2"

        self.df_loss_raw = self.df_loss_raw

    def replicate_lines(self, df):
        """
        Some photos on the webpage include several equipment items.
        To count losses properly we need to split such entries, so
        we have one row per each equipment item lost.
        """

        # Initiate a copy of data frame index
        counter_idx = 0

        # Create an empty data frame with the same structure as df
        # TODO: Probably not effective but anyways takes fractions of second
        df_replicas = df[0:0]

        # Loop through the input data frame and create a data frame with replicated rows
        # TODO: Looks not nice but will do for now
        for i in df["Number of IDs"]:
            if i != 1:
                df_replicas = pd.concat([df_replicas, pd.concat([df.iloc[[counter_idx]]] * (i - 1),
                                                                ignore_index=True)], ignore_index=True)
            counter_idx += 1

        # Combine input data frame with replica data frame
        df_final = pd.concat([df, df_replicas], ignore_index=True)
        df_final.sort_values(by=["Equipment Category", "Equipment Model", "Action ID & Types",
                                 "Source Link Original"], inplace=True)
        df_final.reset_index(inplace=True, drop=True)

        return df_final

    def extract_date_txt_from_twit(self):
        date_txt = ""
        # We need to extract a <span>, which comes right after <a>, which contains link to a current
        # webpage (href_id). This <span> contains information about the date twit was posted.
        href_id = self.web_url.replace("https://twitter.com", "").replace("http://twitter.com", "")
        try:
            date_txt = self.html_soup.find("a", href=href_id).span.text.strip()
        except Exception:
            pass
        return date_txt


class ImageRecognizer:
    """
    Class to take an image and recognize a date from it.
    """

    def __init__(self, path_tsr: str):
        """
        :param path_tsr: Path to Tesseract engine .exe
        """

        self.path_tsr = path_tsr

        # Avoid website certificate validation errors
        ssl._create_default_https_context = ssl._create_unverified_context

        # Point pytesseract to the Tesseract engine's .exe
        pytesseract.tesseract_cmd = self.path_tsr

    def parse_txt_from_img(self, img_url, psm=11, whitelist=False, adjust_img=False, invert_img=False,
                           black_white=False):
        """
        Parse a web image to retrieve all the text that contains a date.
        """

        # Tesseract has different page segmentation modes that might improve on recognition results.
        # The ones worked the best for us are 6, 11, 12.
        # Page segmentation modes:
        #    - 0 => Orientation and script detection (OSD) only
        #    - 1 => Automatic page segmentation with OSD
        #    - 2 => Automatic page segmentation, but no OSD, or OCR
        #    - 3 => Fully automatic page segmentation, but no OSD (Default)
        #    - 4 => Assume a single column of text of variable sizes
        #    - 5 => Assume a single uniform block of vertically aligned text
        #    - 6 => Assume a single uniform block of text
        #    - 7 => Treat the image as a single text line
        #    - 8 => Treat the image as a single word
        #    - 9 => Treat the image as a single word in a circle
        #    - 10 => Treat the image as a single character
        #    - 11 => Sparse text. Find as much text as possible in no particular order
        #    - 12 => Sparse text with OSD
        #    - 13 => Raw line. Treat the image as a single text line, bypassing hacks that are Tesseract-specific

        # Check psm argument
        if psm not in range(0, 14):
            err_msg = "Please provide Page Segmentation Mode as an integer between 0 and 13"
            return err_msg

        # Defining whitelist on this stage might cause errors in date recognition
        # Consider below scenario:
        #  - w/o whitelist:
        #      * raw text: "a2bcd3/7/2022"
        #      * date recognized: "03/07/2022"
        #  - with whitelist:
        #      * raw text: "23/7/2022"
        #      * date recognized: "23/07/2022"
        # Running recognition two times (with and w/o whitelist) is a waste of time.
        # Therefore, if we would want to see a cleaned-up text in output it's better
        # to use regex (i.e. "[^\d|\/|:|-]").

        # Define whitelist
        if whitelist:
            whitelist = "-c tessedit_char_whitelist=0123456789.:-/"
        else:
            whitelist = ""

        # Ad-hoc tests show that the way you open a file impacts the content
        # of a recognized text but does not affect recognition performance
        # (i.e. for both "open" methods tesseract recognized and didn't
        # recognize text from the same images). Tests obviously were not
        # throughout but gor our purposes let's assume that the method of
        # opening the image doesn't matter. Previous method:
        # urlretrieve(img_url, "file_img"); img = Image.open("file_img")

        # Open an image
        try:
            img = imutils.url_to_image(img_url)
        except Exception as e:
            err_msg = repr(e)
            return err_msg
        if img is None:
            err_msg = "Image downloaded is None"
            return err_msg

        # Invert image for a better recognition performance
        if invert_img:
            img = np.invert(img)

        # Convert all non-black pixels to a white color
        # Most dates are added to the picture as an overlay in black color
        # This method works quite well & removes a lot of noise, so is our default
        # https://localcoder.org/replace-all-the-colors-of-a-photo-except-from-the-existing-black-and-white-pixel
        if black_white:
            img[img != 0] = 255

        # Adjust image for better recognition performance
        # https://stackoverflow.com/questions/66856172/using-tesseract-to-read-dates-from-a-small-images
        if adjust_img:
            img = cv2.resize(img, (img.shape[1] * 10, img.shape[0] * 10), interpolation=cv2.INTER_AREA)
            img = cv2.copyMakeBorder(img, 50, 50, 50, 50, cv2.BORDER_CONSTANT, value=[0, 0, 0])
            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            img = cv2.dilate(img, None, iterations=1)
            # _, img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)

        # Show image
        # cv2.imshow('image',img)
        # cv2.waitKey(0)

        # Recognize text from the image
        img_txt = pytesseract.image_to_string(img, config=fr'{whitelist} --psm {psm}')

        # Get rid of white spaces
        p = re.compile(r"\s+")
        img_txt = re.sub(p, '', img_txt)

        return img_txt


class DateParser:
    """
    Class to convert strings into dates.
    """

    def check_date(self, date_dt):
        if date_dt is None:
            # parse_date() returns None if all 5 methods of date recognitions did not work
            check = False
        else:
            # Dates must be in range between 24.02.2022 (start of war) and today
            date_war_start = datetime(2022, 2, 24).date()
            date_today = datetime.now().date()
            check = date_war_start <= date_dt <= date_today
        return check

    def convert_txt_to_date(self, date_txt):

        # Prerequisites:
        #   - Dates can be in different formats (YYYY-MM-DD, MM-DD-YYYY, M-D-YY etc.)
        #   - By default we assume European date format (day first)

        # parse() arguments:
        #  - dayfirst – Whether to interpret the first value in an ambiguous 3-integer
        #    date (e.g. 01/05/09) as the day (True) or month (False). If yearfirst is set
        #    to True, this distinguishes between YDM and YMD. If set to None, this value is
        #    retrieved from the current parserinfo object (which itself defaults to False).
        #  - yearfirst – Whether to interpret the first value in an ambiguous 3-integer date
        #    (e.g. 01/05/09) as the year. If True, the first number is taken to be the year,
        #    otherwise the last number is taken to be the year. If this is set to None, the
        #    value is retrieved from the current parserinfo object (which itself defaults to False).

        def parse_date(txt, dayfirst, yearfirst):
            """
            Function added for convenience, so not to repeat try-except construct all the time.
            """
            try:
                ret_date = parser.parse(txt, ignoretz=True, dayfirst=dayfirst, yearfirst=yearfirst).date()
            except Exception:
                ret_date = None
            return ret_date

        # Date parser doesn't recognize "_", so replacing with "-"
        date_txt = date_txt.replace("_", "-")

        # Parse unknown date format
        converted_dt = parse_date(date_txt, dayfirst=True, yearfirst=False)
        if self.check_date(converted_dt):
            pass
        else:
            converted_dt = parse_date(date_txt, dayfirst=False, yearfirst=True)
            if self.check_date(converted_dt):
                pass
            else:
                converted_dt = parse_date(date_txt, dayfirst=True, yearfirst=True)
                if self.check_date(converted_dt):
                    pass
                else:
                    converted_dt = parse_date(date_txt, dayfirst=False, yearfirst=False)
                    if self.check_date(converted_dt):
                        pass
                    else:
                        converted_dt = None

        return converted_dt

    def parse_date_from_txt(self, input_txt, src):

        # Check src argument
        if src not in ["pic", "twit"]:
            src = "pic"

        # Test cases:
        #   * pic:
        #       - xsxsxs01/02/2022
        #       - 1/2/2022---xsxsxs
        #       - xsxs1/2/22xsxs
        #       - 01/2/22
        #       - 1/02/2022--xsxsxs
        #       - xsxsxsxx1-2-2022
        #       - 12 01 2022xsxsxs
        #       - 2022-01-02
        #       - 2022-1-3
        #       - 2022:12:21
        #       - 2022 12 01
        #   * twit:
        #       - 5:09 AM · Feb 27, 2022

        if src == "pic":
            # Since we are removing all whitespaces (including \n) in ImageRecognizer.parse_txt_from_img()
            # we can use \W instead of [^a-zA-Z0-9_\n].
            # Below two patterns are combined with OR (|).
            # Day and month can be coded as one or two digits.
            # DD-MM-YYYY / DD-MM-YY ==> (\d{1,2}\W\d{1,2}\W(\d{4}|\d{2}))
            # YY-MM-DD / YYYY-MM-DD ==> ((\d{2}|\d{4})\W\d{1,2}\W\d{1,2})
            pattern_txt = r"(\d{1,2}\W\d{1,2}\W(\d{4}|\d{2}))|((\d{2}|\d{4})\W\d{1,2}\W\d{1,2})"
        elif src == "twit":
            pattern_txt = r"[a-zA-Z]{3} \d{1,2}, \d{4}"

        p = re.compile(pattern_txt)
        date_txt_match = p.search(input_txt)
        if date_txt_match is None:
            date_dt = None
        else:
            date_txt = date_txt_match.group()
            date_dt = self.convert_txt_to_date(date_txt)

        return date_dt


class DataReconciliation:
    """
    Class to handle Postgres database connection and data update.
    """

    def __init__(self, username, password, host, port, db_name):

        self.nd = None
        self.update_data = None
        self.od_cat = None
        self.od_type = None
        self.countries = None
        self.od_item = None
        self.curr_type = None
        self.reconciled_df = None

        # Connect to Postgres DB
        conn_string = "postgresql+psycopg2://postgres:" + username + ":" + password + "@" + \
                      host + ":" + port + "//" + db_name
        self.engine = create_engine(conn_string)

    def update_db_step_1(self, df_new):
        """
        Update database with a new data (equipment categories and models).
        """

        # Define new data
        self.nd = df_new

        # Extract required data from a database
        self.extract_data_from_db()

        # Data to be compared during reconciliation:
        # old_data (from the db) = od
        # new_data (from the parser) = nd

        # We update database in the following order:
        # 1. Category
        # 2. Models
        # 3. Items

        self.update_eq_categories()
        self.update_eq_models()
        self.reconcile_eq_items()

    def update_db_step_2(self, df_new):
        """
        Update database with a new data (daily equipment losses).
        """

        # Redefine new data (it contains now a column with Event Date)
        self.update_data = df_new

        # Update db with daily losses
        self.update_eq_items()

    def extract_data_from_db(self):
        # Get a list of all equipment categories currently in use (from equipment_categories)
        self.od_cat = pd.read_sql_query('select * from public.equipment_categories', con=self.engine)
        # Get the list of all equipment models currently in use
        self.od_type = pd.read_sql_query('select * from public.equipment_types_decoded', con=self.engine)
        # Get the list of all equipment categories currently in use (from countries_orgs)
        self.countries = pd.read_sql_query('select * from public.countries_orgs', con=self.engine)
        # Get the list of all items currently reported in logs
        self.od_item = pd.read_sql_query('select * from public.daily_losses_log_decoded', con=self.engine)
        # Get the list of all equipment models currently in use
        self.curr_type = pd.read_sql_query('select * from public.equipment_types', con=self.engine)

    def update_eq_categories(self):
        """
        Reconcile and update a list of equipment categories.
        """

        # Get a list of all equipment categories currently in use (from equipment_categories)
        od_cat = self.od_cat

        # Create a list of unique equipment categories for new data
        cat_upd = self.nd[['Equipment Category', 'Equipment Type']].drop_duplicates()

        # Reconcile new vs old data:
        # 'Equipment Category' = 'c_eng_name'
        cat_upd = pd.merge(cat_upd, od_cat, how='left', left_on='Equipment Category', right_on='c_eng_name')
        # Keep only items not yet present in the database
        cat_upd.fillna('New', inplace=True)
        cat_upd = cat_upd[cat_upd.c_eng_name == 'New']

        # Feed equipment_categories table w/ unique categories
        if cat_upd.shape[0] > 0:
            for index, row in cat_upd.iterrows():
                self.engine.execute("insert into public.equipment_categories(c_type, c_eng_name) values "
                                    "('" + row["Equipment Type"] + "','" + row["Equipment Category"] + "')")

        # Report progress
        print('######### Added ' + str(cat_upd.shape[0]) + ' items to the categories')

        # Get updated od_cat df
        self.od_cat = pd.read_sql_query('select * from public.equipment_categories', con=self.engine)

    def update_eq_models(self):
        """
        Reconcile and update a list of equipment models.
        """

        # Get the list of all equipment models currently in use
        od_type = self.od_type
        # Get the list of all equipment categories currently in use (from countries_orgs)
        countries = self.countries
        # Get a list of all equipment categories currently in use (from equipment_categories)
        od_cat = self.od_cat

        # Create a key for the old data
        od_type['key'] = od_type['category_l2_eng_encoded'].str.lower() + od_type['series_number'].str.lower() + \
                         od_type['country_of_origin_name'].str.lower()
        od_type['key'] = od_type['key'].str.replace(r'[^\d\w]', '', regex=True)

        # Create df for the parsed data with unique equipment models
        type_upd = self.nd[['Equipment Category', 'Equipment Model', 'Equipment Producer']].drop_duplicates()
        # Change column names to match old data
        rename_cols = {'Equipment Producer': 'country_of_origin_name', 'Equipment Category': 'category_l2_eng_encoded',
                       'Equipment Model': 'series_number'}
        type_upd.rename(columns=rename_cols, inplace=True)
        # Create a key for the new data
        type_upd['key'] = type_upd['category_l2_eng_encoded'].str.lower() + type_upd['series_number'].str.lower() + \
                          type_upd['country_of_origin_name'].str.lower()
        type_upd['key'] = type_upd['key'].str.replace(r'[^\d\w]', '', regex=True)

        # Reconcile new vs old data
        type_upd = pd.merge(type_upd, od_type, how='left', on='key')
        # Drop only those cases where the type does not exist in the old data
        type_upd.fillna('New', inplace=True)
        type_upd = type_upd[type_upd.series_number_y == 'New']

        # Do 2-steps reconciliation
        # Source foreign key = country_of_origin_id
        type_upd = pd.merge(type_upd, countries, how='left', left_on='country_of_origin_name_x', right_on='full_name')
        # Source foreign key = equipment_categories_id
        type_upd = pd.merge(type_upd, od_cat, how='left', left_on='category_l2_eng_encoded_x', right_on='c_eng_name')

        # Get rid of unnecessary columns and rename them
        type_upd = type_upd[['id_y', 'series_number_x', 'id_x']]
        type_upd = type_upd.rename(columns={'id_y': 'category_type_id', 'series_number_x': 'series_number',
                                            'id_x': 'country_of_origin_id'})

        # Feed equipment_categories table w/ unique categories
        if type_upd.shape[0] > 0:
            for index, row in type_upd.iterrows():
                self.engine.execute("insert into public.equipment_types(category_type_id, series_number, country_of_origin_id) "
                                    "values (" + str(row.category_type_id) + ",'" + str(row.series_number) + "'," +
                                    str(row.country_of_origin_id) + ")")

        # Report progress
        print('######### Added ' + str(type_upd.shape[0]) + ' items to the models')

    def reconcile_eq_items(self):
        """
        Reconcile a list of equipment losses.
        """

        # Get the list of all items currently reported in logs
        od_item = self.od_item

        # Change column names
        rename_cols = {'source_link_original': 'link', 'country_name': 'country', 'c_eng_name': 'category',
                       'series_number': 'type', "impact_type": "impact"}
        od_item.rename(columns=rename_cols, inplace=True)

        # Create a key for the old data
        od_item['key'] = od_item['country'].str.lower() + od_item['category'].str.lower() + \
                         od_item['type'].str.lower() + od_item['link'].str.lower() + od_item['impact'].str.lower()
        od_item['key'] = od_item['key'].str.replace(r'[^\d\w]', '', regex=True)

        # Get rid of duplicates (WebParser.replicate_lines() splits one row into few -
        # we do not need that for reconciliation)
        od_item.drop_duplicates(subset="key")

        # Create df for the parsed data (this data is before WebParser.replicate_lines() was applied)
        item_upd = self.nd

        # Get a list of parsed data column names
        item_upd_col_names = list(item_upd.columns)

        # Create a key for the new data
        item_upd['key'] = item_upd['Impacted Country'].str.lower() + item_upd['Equipment Category'].str.lower() + \
                          item_upd['Equipment Model'].str.lower() + item_upd['Source Link Original'].str.lower() + \
                          item_upd['Action Type'].str.lower()
        item_upd['key'] = item_upd['key'].str.replace(r'[^\d\w]', '', regex=True)

        # Reconcile new vs old data
        item_upd = pd.merge(item_upd, od_item, how='left', on='key')
        # Drop only those cases where the item log does not exist in the old data
        item_upd.fillna('New', inplace=True)
        item_upd = item_upd[item_upd.link == 'New']

        self.reconciled_df = item_upd[item_upd_col_names]

    def update_eq_items(self):
        """
        Update a list of equipment losses.
        """

        # Get the list of all equipment categories currently in use (from countries_orgs)
        countries = self.countries
        # Get a list of all equipment categories currently in use (from equipment_categories)
        od_cat = self.od_cat

        # Create df for the reconciled data (this data is before WebParser.replicate_lines() was applied)
        item_upd = self.update_data

        # Source foreign key = country_of_origin_id (-> id)
        item_upd = pd.merge(item_upd, countries, how='left', left_on='Impacted Country', right_on='full_name')
        item_upd.rename(columns={'id': 'country_id'}, inplace=True)

        # Source category data
        item_upd = pd.merge(item_upd, od_cat, how='left', left_on='Equipment Category', right_on='c_eng_name')
        item_upd.rename(columns={'id': 'category_id'}, inplace=True)

        # Get the list of all equipment types currently in use
        curr_type = self.curr_type

        # Source model data
        item_upd = pd.merge(item_upd, curr_type, how='left', left_on='Equipment Model', right_on='series_number')
        item_upd.rename(columns={'id': 'series_number_id'}, inplace=True)

        # Add conflict metadata
        item_upd['conflict_id'] = 1
        item_upd['source_name'] = 'Oryx'

        # Feed equipment_categories table w/ unique categories
        if item_upd.shape[0] > 0:
            for index, row in item_upd.iterrows():
                self.engine.execute(
                    "insert into public.daily_losses_log(conflict_id, impacted_side_id, impact_type, equipment_category, "
                    "equipment_type, source_name, source_link_original, source_link_final, date) values (" +
                    str(row.conflict_id) + "," + str(row.country_id) + ",'" + row["Action Type"] + "'," +
                    str(row.category_id) + "," + str(row.series_number_id) + ",'" + row.source_name + "','" +
                    row["Source Link Original"] + "','" + row["Source Link Final"] + "','" + row["Event Date"] + "')")

        # Report progress
        print('######### Added ' + str(item_upd.shape[0]) + ' items to the items')
